


## 推理脚本规范
-	推理脚本需要实现 tiinfer.Model 类对应的函数，包含 init 初始化、load 模型加载、preprocess 数据预处理、predict 模型推理、postprocess 后处理函数。
-	初始化函数 init 中，根据 model_dir 参数可以获取代码运行的绝对路径目录，如模型加载需要绝对路径，需要根据 model_dir 参数和目录内模型的相对路径拼接获取绝对路径，其他配置文件的绝对路径类似。
- 当使用优化模型进行推理时，需要引入 tiacc-inference 加速库。
- 如果使用的是 Detectron2 或者 MMDetection 格式的优化模型，需要使用 [tiacc-inference.load()](#tiaccinferenceload) 函数对模型进行加载，其他格式不涉及。
-	服务启动默认是单进程的，如果想要多进程，可以通过在 init 函数中设置 self.workers 参数，如 self.workers=2 会自动启动两个进程的推理服务。

## 分类模型推理脚本 Pytorch 实现示例
```
import os
from typing import Dict
import logging
import torch
import torchvision.models
import torch.nn.functional as F
from torchvision import transforms
import numpy as np
import cv2
import tiinfer
import base64
import urllib
import time
from torchvision.io.image import decode_jpeg
import pdb

PYTORCH_FILE = "models/model.pth"


class ClassifyModel(tiinfer.Model):
    def __init__(self, model_dir: str):
        super().__init__(model_dir)
        self.model_dir = model_dir
        self.workers = 1
        self.model_name = "inception"
        self.result_type = "classification"
        self.ready = False
        self.model = None
        self.use_gpu = torch.cuda.is_available()
        self.device = torch.device('cuda:0' if self.use_gpu else 'cpu')
        self.resize = transforms.Resize((224, 224))
        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])
        self.decode_tensor = True


    def load(self) -> bool:
        try:
            # Load .pth model file
            model_file = os.path.join(self.model_dir, PYTORCH_FILE)
            state_dict = torch.load(model_file, map_location=self.device)
            # Load the python class into memory
            model_class = torchvision.models.resnet50(pretrained=False)
            # Modify last FC
            num_classes = state_dict['model_state_dict']['fc.weight'].shape[0]
            model_class.fc = torch.nn.Linear(
                model_class.fc.in_features, num_classes)
            # Make sure the model weight is transform with the right device in this machine
            self.train_classes = state_dict['train_classes']
            self.model = model_class.to(self.device)
            logging.info("load model to device %s" % (self.device) )
            self.model.load_state_dict(
                ({k.replace('module.', ''): v for k, v in state_dict['model_state_dict'].items()}))
            self.model.eval()
        except Exception as e:  # pylint: disable=broad-except
            logging.error("load model failed: %s", e)
            self.ready = False
            return self.ready
        self.ready = True
        return self.ready

    def preprocess(self, request: Dict) -> Dict:
        try:
            start_time = time.time()
            if "images" not in request:
                return {"error": "invalid param for has no images key"}
            image_size_array = []
            images = request["images"]
            inputs=[]
            for _, img_data in enumerate(images):
                if os.path.isfile(img_data):
                    img_mat = cv2.imread(img_data)
                    img_mat = torch.Tensor(img_mat).permute((2,0,1))
                elif img_data.startswith("http"):
                    url_response = urllib.request.urlopen(img_data)
                    image_data = url_response.read()
                    image_nparr = np.array(bytearray(image_data), dtype=np.uint8)
                    if self.decode_tensor:
                        torch_byte =torch.from_numpy(image_nparr)
                        img_mat = decode_jpeg(torch_byte, device = self.device)
                    else:
                        img_mat = cv2.imdecode(image_nparr, cv2.IMREAD_COLOR)
                        img_mat = torch.Tensor(img_mat).permute((2,0,1))
                else:
                    image_data = base64.b64decode(img_data)
                    image_nparr = np.frombuffer(image_data, np.uint8)
                    if self.decode_tensor:
                        torch_byte =torch.from_numpy(image_nparr)
                        img_mat = decode_jpeg(torch_byte, device = self.device)
                    else:
                        img_mat = cv2.imdecode(image_nparr, cv2.IMREAD_COLOR)
                        img_mat = torch.Tensor(img_mat).permute((2,0,1))
                # pdb.set_trace()
                img_tensor = self.normalize(self.resize(img_mat).type(torch.float32)/255)
                inputs.append(img_tensor.unsqueeze(0))
            logging.info("preprocess cost: %.2f ms", 1000*(time.time() - start_time))
            return {"instances": inputs, "original_dims": image_size_array}
        except Exception as e:  # pylint: disable=broad-except
            logging.error("Preprocess failed: %s" % (e))
            return {"error": "preprocess failed"}

    def predict(self, request: Dict) -> Dict:
        if "instances" not in request:
            return request
        inputs = []
        with torch.no_grad():
            try:
                inputs = torch.cat(request["instances"], dim=0)
                pred = self.model(inputs)
                return {"predictions":  pred}
            except Exception as e:  # pylint: disable=broad-except
                logging.error("Failed to predict" % (e))
                request.pop("image", '')
                request.pop("instances", '')
                request["predict_err"] = str(e)
                return request

    def postprocess(self, request: Dict) -> Dict:
        if "predictions" not in request:
            return request
        try:
            output = request["predictions"]
            prob = F.softmax(output, dim=1)
            return {"result": {
                "result_type": self.result_type,
                "model_name": self.model_name,
                "categories": self.train_classes,
                "confidence": prob.tolist()
            }}
        except Exception as e:  # pylint: disable=broad-except
            logging.error("Postprocess failed: %s" % (e))
            request.pop("image", '')
            request.pop("instances", '')
            request.pop("predictions", '')
            request["post_process_err"] = str(e)
            return request 
```


## 优化后的分类模型推理脚本 Pytorch 实现示例

```
import os
from typing import Dict
import logging
import torch
import torchvision.models
import torch.nn.functional as F
from torchvision import transforms
import numpy as np
import cv2
import tiinfer
import base64
import urllib
import time
from torchvision.io.image import decode_jpeg
import tiacc_inference
import pdb


PYTORCH_FILE = "models/tiacc.pt"


class ClassifyModel(tiinfer.Model):
    def __init__(self, model_dir: str):
        super().__init__(model_dir)
        self.model_dir = model_dir
        self.workers = 1
        self.model_name = "inception"
        self.result_type = "classification"
        self.ready = False
        self.model = None
        self.use_gpu = torch.cuda.is_available()
        self.device = torch.device('cuda:0' if self.use_gpu else 'cpu')
        self.resize = transforms.Resize((224, 224))
        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])
        self.decode_tensor = True


    def load(self) -> bool:
        try:
            # Load .pth model file
            model_file = os.path.join(self.model_dir, PYTORCH_FILE)
            model = torch.jit.load(model_file, map_location=self.device)
            self.model = model
            logging.info("load model to device %s" % (self.device) )
            self.model.eval()
            self.train_classes = []
        except Exception as e:  # pylint: disable=broad-except
            logging.error("load model failed: %s", e)
            self.ready = False
            return self.ready
        self.ready = True
        return self.ready

    def preprocess(self, request: Dict) -> Dict:
        try:
            start_time = time.time()
            if "images" not in request:
                return {"error": "invalid param for has no images key"}
            image_size_array = []
            images = request["images"]
            inputs=[]
            for _, img_data in enumerate(images):
                if os.path.isfile(img_data):
                    img_mat = cv2.imread(img_data)
                    img_mat = torch.Tensor(img_mat).permute((2,0,1))
                elif img_data.startswith("http"):
                    url_response = urllib.request.urlopen(img_data)
                    image_data = url_response.read()
                    image_nparr = np.array(bytearray(image_data), dtype=np.uint8)
                    if self.decode_tensor:
                        torch_byte =torch.from_numpy(image_nparr)
                        img_mat = decode_jpeg(torch_byte, device = self.device)
                    else:
                        img_mat = cv2.imdecode(image_nparr, cv2.IMREAD_COLOR)
                        img_mat = torch.Tensor(img_mat).permute((2,0,1))
                else:
                    image_data = base64.b64decode(img_data)
                    image_nparr = np.frombuffer(image_data, np.uint8)
                    if self.decode_tensor:
                        torch_byte =torch.from_numpy(image_nparr)
                        img_mat = decode_jpeg(torch_byte, device = self.device)
                    else:
                        img_mat = cv2.imdecode(image_nparr, cv2.IMREAD_COLOR)
                        img_mat = torch.Tensor(img_mat).permute((2,0,1))
                # pdb.set_trace()
                img_tensor = self.normalize(self.resize(img_mat).type(torch.float32)/255)
                inputs.append(img_tensor.unsqueeze(0))
            logging.info("preprocess cost: %.2f ms", 1000*(time.time() - start_time))
            return {"instances": inputs, "original_dims": image_size_array}
        except Exception as e:  # pylint: disable=broad-except
            logging.error("Preprocess failed: %s" % (e))
            return {"error": "preprocess failed"}

    def predict(self, request: Dict) -> Dict:
        if "instances" not in request:
            return request
        inputs = []
        with torch.no_grad():
            try:
                inputs = torch.cat(request["instances"], dim=0)
                pred = self.model(inputs)
                return {"predictions":  pred}
            except Exception as e:  # pylint: disable=broad-except
                logging.error("Failed to predict" % (e))
                request.pop("image", '')
                request.pop("instances", '')
                request["predict_err"] = str(e)
                return request

    def postprocess(self, request: Dict) -> Dict:
        if "predictions" not in request:
            return request
        try:
            output = request["predictions"]
            prob = F.softmax(output, dim=1)
            return {"result": {
                "result_type": self.result_type,
                "model_name": self.model_name,
                "categories": self.train_classes,
                "confidence": prob.tolist()
            }}
        except Exception as e:  # pylint: disable=broad-except
            logging.error("Postprocess failed: %s" % (e))
            request.pop("image", '')
            request.pop("instances", '')
            request.pop("predictions", '')
            request["post_process_err"] = str(e)
            return request

```


## TI-ACC-推理加速函数介绍
[](id:tiaccinferenceload)
#### tiacc_inference.load()
TI-ONE 的 Detectron2 或者 MMDetection 格式优化后的模型，需要使用 tiacc-inference.load() 函数对模型进行加载，其他格式无需使用tiacc_inference 的 load，使用原生格式 load 即可。
- MMDetection
原生示例：
```
from mmdet.apis import init_detector
model = init_detector(config, checkpoint, device=device)
```
优化后使用示例：
```
import tiacc_inference
model = tiacc_inference.load('tiacc.pt')  #tiacc.pt是模型优化后生成的新模型
```

- Detectro2(对于 Detectron2 导出的 PyTorch 模型)
原生示例：
```
import torch
model = torch.load(checkpoint) #.pth模型文件
```
优化后使用示例:
```
import tiacc_inference
model = tiacc_inference.load('tiacc.pt')  #tiacc.pt是模型优化后生成的新模型
```

- Detectro2(对于通过 Detectron2.modeling.build_model 构造的模型)
原生示例：
```
from detectron2.config import get_cfg
from detectron2.modeling import build_model
cfg = get_cfg()
cfg.MODEL.DEVICE = device
cfg.MODEL.WEIGHTS = checkpoint
model = build_model(cfg)
```
优化后使用示例:
```
import tiacc_inference
model = tiacc_inference.load('tiacc.pt') #tiacc.pt是模型优化后生成的新模型
```



