模型仓库支持导入模型。列表页管理着不同的模型以及模型的不同版本；导入窗口支持从任务式建模直接导入模型以及从 COS 上传。

## 仓库列表

模型仓库列表中可查看模型及对应的版本，并对模型进行发布封装等操作，分为模型列表和优化模型列表，模型列表为客户上传上来的原始模型列表，优化模型列表为经过模型优化后保存的优化后的模型列表。
![](https://qcloudimg.tencent-cloud.cn/raw/3676b3021b3c3982e2c22c00630adee2.png)
- 支持对模型进行删除，单击**删除**按钮，此模型及对应的模型版本皆被删除。
- 支持对标签进行编辑，单击**编辑标签**按钮，可进行标签编辑。
- 支持重新上传模型相关文件，单击**上传文件**进行编辑模型文件，需要注意的是，重新编辑的模型相关文件需要在服务更新后才能生效。
- 支持发布封装，单击**发布封装**进行模型服务发布。
- 单击版本对应的**删除**按钮可删除对应的模型版本，需要注意的是，删除后不可恢复，已经启动的模型服务停止后将无法自动重新启动。删除时可单击**查看关联服务组**进行查看关联的模型服务组以确定是否要进行删除。
- 自动学习目标检测和图片分类支持下载 SDK，**下载 sdk** 按钮可打开 COSBrowser 进行下载。您需要先安装 COSBrowser 客户端。


## 导入模型

进入**模型管理** > **模型仓库**，单击**导入模型** 按钮开始导入模型。



### 导入新模型
<img src="https://qcloudimg.tencent-cloud.cn/raw/56f0ef0e50ead2461f847c94e8781490.png" width="70%">

1. 按要求填入模型名称、自定义设置标签。（标签用于从不同维度对资源分类管理。用户需要先从控制台进行 [标签设置](https://console.cloud.tencent.com/tag/taglist)。对于标签设置您可以直接单击 **标签管理使用指南**查看。）
2. 从任务导入：
  - 选择训练任务名称。
  - 选择训练任务之后，然后选择对应的模型格式，如有任务对应的算法框架、模型来源路径、模型指标信息，信息将会被自动填充，模型指标填充的是最新上报指标。
  - 运行环境您可选择内置的，也可通过自定义选择的方式从容器镜像服务中拉取，优化后的模型目前仅支持内置运行环境运行。
  - 模型文件也会自动获取此任务的模型文件，您可选择剪切或者复制的方式对此模型文件进行处理，处理完成后将自动为您生成 COS 路径。
3. 从 COS 导入：
  - 选择模型格式，目前模型格式支持 Savedmodel、Frozen Graph、TorchScript、Detectron2、PyTroch 原生、PMML、ONNX、MMDetection。
  - 输入您期望上传的模型对应的模型指标
  - 运行环境您可选择内置的，支持内置的运行环境对应上述支持的模型格式，也可通过自定义选择的方式从容器镜像服务中拉取。
  - 上传模型文件：需要您上传模型文件、推理代码和配置文件。推理脚本代码和服务 API 配置文件请分别命名为 config.json、model_service.py，具体请参考 [模型包规范](https://cloud.tencent.com/document/product/851/74145) 和 [推理脚本代码示例](https://cloud.tencent.com/document/product/851/74148)。
 

###  导入新版本
<img src="https://qcloudimg.tencent-cloud.cn/raw/c15b0d287f7837a58c55d2fbc3fba3d5.png" width="70%">

1. 您可选择一个模型后导入这个模型的新版本，模型版本号将自动在历史最高版本号基础上+1，如V2、V3等。
2. 从任务导入：
	-	选择训练任务名称。
	-	选择训练任务之后，然后选择对应的模型格式，如有任务对应的算法框架、模型来源路径模型指标信息，信息将会被自动填充，模型指标填充的是最新上报指标。
	-	运行环境您可选择内置的，也可通过自定义选择的方式从容器镜像服务中拉取，优化后的模型目前仅支持内置运行环境运行
	-	模型文件也会自动获取此任务的模型文件，您可选择剪切或者复制的方式对此模型文件进行处理，处理完成后将自动为您生成 COS 路径。
3. 从 COS 导入：
	-	选择模型格式，目前模型格式支持 Savedmodel、Frozen Graph、TorchScript、Detectron2、PyTroch原生、PMML、ONNX、MMDetection。
	-	输入您期望上传的模型对应的模型指标
	-	运行环境您可选择内置的，支持内置的运行环境对应上述支持的模型格式，也可通过自定义选择的方式从容器镜像服务中拉取。
	-	上传模型文件：需要您上传模型文件、推理脚本代码和服务 API 配置文件。推理代码和配置文件请分别命名为 config.json、model_service.py ，具体请参考 [模型包规范](https://cloud.tencent.com/document/product/851/74145) 和 [推理脚本代码示例](https://cloud.tencent.com/document/product/851/74148)。

完成以上配置后，单击**确定**按钮，模型便导入成功。


## 模型优化
原始模型列表的模型可在模型优化模块进行推理加速，请在参考 [模型优化](https://cloud.tencent.com/document/product/851/74676) 对模型进行推理加速。

## 发布封装
原始模型在原始模型列表对应的模型版本操作中，单击**发布封装**进行服务的发布封装，优化模型在原始模型列表对应的模型版本操作中，单击**发布封装**进行服务的发布封装，后续详细步骤请参考 [模型服务](https://cloud.tencent.com/document/product/851/74139) 对模型进行部署和使用。















